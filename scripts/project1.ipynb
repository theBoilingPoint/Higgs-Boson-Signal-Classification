{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "Y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from cross_validation import *\n",
    "from preprocess import *\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\t        train loss: 0.3065653039753832\n",
      "\t        test loss: 0.30892820469425386\n",
      "\t        train acc: 0.781954\n",
      "\t        test acc: 0.7818400000000001\n",
      "\t        train var acc: 2.5513240000000265e-06\n",
      "\t        test var acc: 5.521920000000019e-06\n",
      "\t\n",
      "\n",
      "        train var loss: 1.959380092267875e-06\n",
      "\t        test var loss: 2.1140245696062797e-05\n",
      "\t        \n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "degree = 3\n",
    "one_hot_enc = True\n",
    "strategy = 'most_freq'\n",
    "\n",
    "gamma = 0.1\n",
    "max_iters = 1000\n",
    "\n",
    "trainer = lambda y, x: least_squares_GD(y, x, initial_w=initial_w, max_iters=max_iters,gamma=gamma)\n",
    "\n",
    "y, tx_train, _ = preprocess(Y, tX, degree=degree, strategy=strategy, one_hot_enc=True, train=True)\n",
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "\n",
    "run_k_fold(y, tx_train, trainer=trainer, compute_loss=compute_mse, k_fold=k_fold, seed=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\t        train loss: 20.308648805083973\n",
      "\t        test loss: 21.784521719073616\n",
      "\t        train acc: 0.566392\n",
      "\t        test acc: 0.567372\n",
      "\t        train var acc: 0.010443671386000003\n",
      "\t        test var acc: 0.010281062176000003\n",
      "\t\n",
      "\n",
      "        train var loss: 244.97894652543442\n",
      "\t        test var loss: 210.74593613097886\n",
      "\t        \n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "degree = 2\n",
    "one_hot_enc = True\n",
    "strategy = 'most_freq'\n",
    "\n",
    "gamma = 0.01\n",
    "max_iters = 1000\n",
    "\n",
    "trainer = lambda y, x: least_squares_SGD(y, x, initial_w=initial_w, max_iters=max_iters,gamma=gamma)\n",
    "\n",
    "y, tx_train, _ = preprocess(Y, tX, degree=degree, strategy=strategy, one_hot_enc=True, train=True)\n",
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "\n",
    "run_k_fold(y, tx_train, trainer=trainer, compute_loss=compute_mse, k_fold=k_fold, seed=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\t        train loss: 0.31514722203195605\n",
      "\t        test loss: 0.37944762192084347\n",
      "\t        train acc: 0.7749179999999999\n",
      "\t        test acc: 0.7746639999999999\n",
      "\t        train var acc: 1.1629600000000736e-07\n",
      "\t        test var acc: 1.0789439999999838e-06\n",
      "\t\n",
      "\n",
      "        train var loss: 8.519226186985896e-08\n",
      "\t        test var loss: 0.015969889273541156\n",
      "\t        \n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "degree = 2\n",
    "one_hot_enc = True\n",
    "strategy = 'most_freq'\n",
    "\n",
    "trainer = lambda y, x: least_squares(y, x)\n",
    "y, tx_train, _ = preprocess(Y, tX, degree=degree, strategy=strategy, one_hot_enc=True, train=True, log=False)\n",
    "run_k_fold(y, tx_train, trainer=trainer, compute_loss=compute_mse, log=False, k_fold=k_fold, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\t        train loss: 0.30778000264512695\n",
      "\t        test loss: 0.3067555006354182\n",
      "\t        train acc: 0.783219\n",
      "\t        test acc: 0.783008\n",
      "\t        train var acc: 9.095400000001107e-08\n",
      "\t        test var acc: 2.753695999999972e-06\n",
      "\t\n",
      "\n",
      "        train var loss: 3.5512923987480334e-08\n",
      "\t        test var loss: 6.13763926601411e-06\n",
      "\t        \n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "degree = 3\n",
    "one_hot_enc = True\n",
    "strategy = 'most_freq'\n",
    "\n",
    "lambda_ = 0.000610540229658532\n",
    "\n",
    "trainer = lambda y, x: ridge_regression(y, x, lambda_=lambda_)\n",
    "y, tx_train, _ = preprocess(Y, tX, degree=degree, strategy=strategy, one_hot_enc=True, train=True, log=False)\n",
    "run_k_fold(y, tx_train, trainer=trainer, compute_loss=compute_mse, log=False, k_fold=k_fold, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\t        train loss: 90936.71779913598\n",
      "\t        test loss: 22746.827701103743\n",
      "\t        train acc: 0.7846409999999999\n",
      "\t        test acc: 0.784464\n",
      "\t        train var acc: 3.942399999999874e-08\n",
      "\t        test var acc: 7.33663999999997e-07\n",
      "\t\n",
      "\n",
      "        train var loss: 4159.592660704778\n",
      "\t        test var loss: 4378.850173873131\n",
      "\t        \n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "degree = 2\n",
    "one_hot_enc = True\n",
    "strategy = 'most_freq'\n",
    "\n",
    "gamma = 1e-6\n",
    "max_iters = 1000\n",
    "\n",
    "log = True\n",
    "threshold = 0.5\n",
    "\n",
    "trainer = lambda y, x: logistic_regression(y,x, initial_w=initial_w, max_iters=max_iters,gamma=gamma)\n",
    "compute_loss = lambda y, x, w: compute_loss_logistic_regression(y, x, w)\n",
    "\n",
    "y, tx_train, _ = preprocess(Y, tX, degree=degree, strategy=strategy, one_hot_enc=True, train=True, log=log)\n",
    "\n",
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "\n",
    "run_k_fold(y, tx_train, trainer=trainer, compute_loss=compute_loss, threshold=threshold, log=log, k_fold=k_fold, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "\t        train loss: 90938.46846015878\n",
      "\t        test loss: 22747.321445882797\n",
      "\t        train acc: 0.784635\n",
      "\t        test acc: 0.784456\n",
      "\t        train var acc: 4.03699999999947e-08\n",
      "\t        test var acc: 7.183039999999815e-07\n",
      "\t\n",
      "\n",
      "        train var loss: 4158.642787075166\n",
      "\t        test var loss: 4377.843362935946\n",
      "\t        \n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "degree = 2\n",
    "one_hot_enc = True\n",
    "strategy = 'most_freq'\n",
    "\n",
    "lambda_ = 1\n",
    "gamma = 1e-6\n",
    "max_iters = 1000\n",
    "\n",
    "log = True\n",
    "threshold = 0.5\n",
    "\n",
    "trainer = lambda y, x: reg_logistic_regression(y, x, initial_w=initial_w, max_iters=max_iters, lambda_=lambda_, gamma=gamma)\n",
    "compute_loss = lambda y, x, w: compute_loss_logistic_regression(y, x, w) + loss_reg_logistic_regression(lambda_, w)\n",
    "\n",
    "y, tx_train, _ = preprocess(Y, tX, degree=degree, strategy=strategy, one_hot_enc=True, train=True, log=log)\n",
    "\n",
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "\n",
    "run_k_fold(y, tx_train, trainer=trainer, compute_loss=compute_loss, threshold=threshold, log=log, k_fold=k_fold, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "_, tX_test = preprocess(_, tX_test, degree=3, strategy='most_freq', train=False, one_hot_enc=True, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../output/logistic_regression_N_Md_D3_1H_lam1_gam1e-06_trainall_2000.csv' \n",
    "y_pred = make_prediction(tX_test, w, threshold=0.5, log=True, test=True)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
